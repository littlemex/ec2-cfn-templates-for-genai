#!/usr/bin/env python3
"""
Multi-Pattern Perfetto Analyzer
è¤‡æ•°Perfettoãƒˆãƒ¬ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã®æ¯”è¼ƒåˆ†æãƒ„ãƒ¼ãƒ«

ãƒ¦ãƒ¼ã‚¶ãƒ¼è¦æ±‚ï¼š
- vmap/scan/for-loopå„ã‚µã‚¤ã‚ºã®è©³ç´°æ¯”è¼ƒåˆ†æ
- è¡¨å½¢å¼ãƒ‡ãƒ¼ã‚¿ï¼ˆTotal slices, TensorMatrixæ™‚é–“, Otheræ“ä½œæ™‚é–“ç­‰ï¼‰ã®JSONå‡ºåŠ›
- ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæº–æ‹ ã®é©å‘½çš„ç™ºè¦‹ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
"""

import sys
import os
import json
import glob
import argparse
from typing import Dict, List, Optional
from pathlib import Path
from dataclasses import dataclass
import subprocess
from perfetto.trace_processor import TraceProcessor

@dataclass
class PatternMetrics:
    """ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¥ãƒ¡ãƒˆãƒªã‚¯ã‚¹"""
    pattern_name: str
    trace_file: str
    file_size_mb: float
    
    # åŸºæœ¬ãƒ¡ãƒˆãƒªã‚¯ã‚¹
    total_slices: int
    unique_operations: int
    unknown_operations: int
    write_operations: int
    event_semaphore: int
    
    # ã‚¨ãƒ³ã‚¸ãƒ³åˆ¥æ™‚é–“ï¼ˆmsï¼‰
    tensormatrix_time_ms: float
    tensormatrix_operations: int
    other_operations_time_ms: float
    other_operations_count: int
    vector_gpsimd_time_ms: float
    vector_gpsimd_operations: int
    scalar_time_ms: float
    scalar_operations: int
    
    # åŠ¹ç‡ãƒ©ãƒ³ã‚¯
    efficiency_rank: int

class MultiPatternPerfettoAnalyzer:
    """è¤‡æ•°ãƒ‘ã‚¿ãƒ¼ãƒ³Perfettoè§£æå™¨"""
    
    def __init__(self, traces_directory: str):
        self.traces_directory = traces_directory
        self.trace_files = self._discover_trace_files()
        self.pattern_metrics = []
        
    def _discover_trace_files(self) -> Dict[str, str]:
        """ãƒˆãƒ¬ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«è‡ªå‹•ç™ºè¦‹ - perfetto_pattern_mapping.jsonå„ªå…ˆä½¿ç”¨"""
        # 1. perfetto_pattern_mapping.jsonã‚’å„ªå…ˆä½¿ç”¨
        mapping_file = os.path.join(self.traces_directory, 'perfetto_pattern_mapping.json')
        if os.path.exists(mapping_file):
            print(f"ğŸ“‹ Using perfetto_pattern_mapping.json: {mapping_file}")
            return self._load_from_mapping_file(mapping_file)
        
        # 2. ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å¾“æ¥ã®globæ¤œç´¢
        print("âš¡ Fallback: Using glob pattern discovery")
        return self._discover_with_glob_patterns()
    
    def _load_from_mapping_file(self, mapping_file: str) -> Dict[str, str]:
        """perfetto_pattern_mapping.jsonã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±ã‚’èª­ã¿è¾¼ã¿"""
        try:
            with open(mapping_file, 'r', encoding='utf-8') as f:
                mapping_data = json.load(f)
            
            files = {}
            perfetto_files = mapping_data.get('perfetto_files', [])
            
            # ãƒ•ã‚¡ã‚¤ãƒ«åãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆæ”¹è‰¯ç‰ˆï¼‰
            pattern_mappings = {
                'vmap': ['vmap'],
                'scan': ['scan'], 
                'for_loop_small': ['for_loop', 'small'],
                'for_loop_medium': ['for_loop', 'medium'],
                'for_loop_large': ['for_loop', 'large']
            }
            
            for pftrace_file in perfetto_files:
                if os.path.exists(pftrace_file):
                    file_name = os.path.basename(pftrace_file)
                    
                    # ãƒ‘ã‚¿ãƒ¼ãƒ³è­˜åˆ¥ï¼ˆæ”¹è‰¯ç‰ˆï¼‰
                    for pattern_name, keywords in pattern_mappings.items():
                        if all(keyword in file_name for keyword in keywords):
                            files[pattern_name] = pftrace_file
                            print(f"âœ… Found {pattern_name}: {file_name}")
                            break
                else:
                    print(f"âš ï¸ File not found: {pftrace_file}")
            
            return files
            
        except Exception as e:
            print(f"âŒ Error loading perfetto_pattern_mapping.json: {e}")
            return self._discover_with_glob_patterns()
    
    def _discover_with_glob_patterns(self) -> Dict[str, str]:
        """å¾“æ¥ã®globãƒ‘ã‚¿ãƒ¼ãƒ³ã§ãƒ•ã‚¡ã‚¤ãƒ«ç™ºè¦‹"""
        files = {}
        
        # äºˆæƒ³ã•ã‚Œã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¿ãƒ¼ãƒ³
        patterns = {
            'vmap': '*vmap*.pftrace',
            'scan': '*scan*.pftrace', 
            'for_loop_small': '*for_loop*small*.pftrace',
            'for_loop_medium': '*for_loop*medium*.pftrace',
            'for_loop_large': '*for_loop*large*.pftrace'
        }
        
        base_path = Path(self.traces_directory)
        
        for pattern_name, file_pattern in patterns.items():
            matches = list(base_path.glob(file_pattern))
            if matches:
                files[pattern_name] = str(matches[0])
                print(f"âœ… Found {pattern_name}: {matches[0].name}")
            else:
                print(f"âš ï¸ Not found {pattern_name}: {file_pattern}")
        
        return files
    
    def analyze_all_patterns(self) -> Dict:
        """å…¨ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æå®Ÿè¡Œ"""
        print(f"ğŸ” Multi-pattern Perfetto Analysis: {self.traces_directory}")
        print(f"ğŸ“ Found {len(self.trace_files)} trace files")
        
        # å„ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åˆ†æ
        for pattern_name, trace_file in self.trace_files.items():
            print(f"\nğŸ“Š Analyzing {pattern_name}...")
            metrics = self._analyze_single_pattern(pattern_name, trace_file)
            self.pattern_metrics.append(metrics)
        
        # åŠ¹ç‡ãƒ©ãƒ³ã‚¯è¨ˆç®—
        self._calculate_efficiency_ranks()
        
        # æ¯”è¼ƒåˆ†æçµæœç”Ÿæˆ
        comparison_result = self._generate_comparison_analysis()
        
        return comparison_result
    
    def _analyze_single_pattern(self, pattern_name: str, trace_file: str) -> PatternMetrics:
        """å˜ä¸€ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ - å®Ÿéš›ã®pftraceãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨"""
        try:
            # ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±å–å¾—
            file_stats = os.stat(trace_file)
            file_size_mb = file_stats.st_size / (1024 * 1024)
            
            # å®Ÿéš›ã®pftraceãƒ‡ãƒ¼ã‚¿ã‹ã‚‰è§£æ
            real_data = self._execute_perfetto_sql_analysis(trace_file)
            return self._convert_to_pattern_metrics(pattern_name, trace_file, file_size_mb, real_data)
                
        except Exception as e:
            print(f"âŒ Error analyzing {pattern_name}: {e}")
            return self._generate_default_metrics(pattern_name, trace_file, 0.0)
    
    def _execute_perfetto_sql_analysis(self, trace_file: str) -> dict:
        """å®Ÿéš›ã®pftraceãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰SQLè§£æå®Ÿè¡Œ - å›ºå®šå€¤ä½¿ç”¨ç¦æ­¢"""
        print(f"ğŸ“Š å®Ÿéš›ã®pftraceãƒ•ã‚¡ã‚¤ãƒ«ã‚’è§£æä¸­: {trace_file}")
        
        try:
            with TraceProcessor(trace=trace_file) as tp:
                # 1. åŸºæœ¬æƒ…å ±å–å¾—
                total_slices_result = tp.query("SELECT COUNT(*) as count FROM slice")
                total_slices = next(iter(total_slices_result)).count
                
                # 2. unknownæ“ä½œæ•°å–å¾—
                unknown_result = tp.query("SELECT COUNT(*) as count FROM slice WHERE name = 'unknown'")
                unknown_operations = next(iter(unknown_result)).count
                
                # 3. å›ºæœ‰æ“ä½œæ•°å–å¾—
                unique_ops_result = tp.query("SELECT COUNT(DISTINCT name) as unique_count FROM slice WHERE name IS NOT NULL")
                unique_operations = next(iter(unique_ops_result)).unique_count
                
                # 4. ã‚¨ãƒ³ã‚¸ãƒ³åˆ¥çµ±è¨ˆå–å¾—
                engine_stats = self._get_engine_statistics_real(tp)
                
                return {
                    'total_slices': total_slices,
                    'unknown_operations': unknown_operations,
                    'tensor_tensor': engine_stats.get('tensor_tensor', {'count': 0, 'total_ms': 0.0}),
                    'tensor_reduce': engine_stats.get('tensor_reduce', {'count': 0, 'total_ms': 0.0}),
                    'matmul': engine_stats.get('matmul', {'count': 0, 'total_ms': 0.0}),
                    'write': engine_stats.get('write', {'count': 0, 'total_ms': 0.0}),
                    'copy': engine_stats.get('copy', {'count': 0, 'total_ms': 0.0}),
                    'dma_operations': engine_stats.get('dma_operations', 0),
                    'event_semaphore': engine_stats.get('event_semaphore', {'count': 0, 'total_ms': 0.0}),
                    'unique_operations': unique_operations
                }
                
        except Exception as e:
            print(f"âŒ Perfettoãƒ•ã‚¡ã‚¤ãƒ«è§£æã‚¨ãƒ©ãƒ¼: {e}")
            raise Exception(f"å®Ÿéš›ã®pftraceãƒ•ã‚¡ã‚¤ãƒ«è§£æã«å¤±æ•—: {e}")
    
    def _get_engine_statistics_real(self, tp) -> dict:
        """å®Ÿéš›ã®pftraceã‹ã‚‰ã‚¨ãƒ³ã‚¸ãƒ³çµ±è¨ˆå–å¾—"""
        queries = {
            'tensor_tensor': "SELECT COUNT(*) as count, COALESCE(SUM(dur)/1e6, 0) as total_ms FROM slice WHERE name = 'TENSOR_TENSOR'",
            'tensor_reduce': "SELECT COUNT(*) as count, COALESCE(SUM(dur)/1e6, 0) as total_ms FROM slice WHERE name = 'TENSOR_REDUCE'",
            'matmul': "SELECT COUNT(*) as count, COALESCE(SUM(dur)/1e6, 0) as total_ms FROM slice WHERE name = 'MATMUL'",
            'write': "SELECT COUNT(*) as count, COALESCE(SUM(dur)/1e6, 0) as total_ms FROM slice WHERE name = 'WRITE'",
            'copy': "SELECT COUNT(*) as count, COALESCE(SUM(dur)/1e6, 0) as total_ms FROM slice WHERE name = 'COPY'",
            'event_semaphore': "SELECT COUNT(*) as count, COALESCE(SUM(dur)/1e6, 0) as total_ms FROM slice WHERE name = 'EVENT_SEMAPHORE'"
        }
        
        engine_stats = {}
        for engine_name, query in queries.items():
            try:
                result = tp.query(query)
                row = next(iter(result))
                engine_stats[engine_name] = {
                    'count': row.count,
                    'total_ms': row.total_ms
                }
            except Exception as e:
                print(f"âš ï¸ ã‚¨ãƒ³ã‚¸ãƒ³çµ±è¨ˆå–å¾—ã‚¨ãƒ©ãƒ¼ ({engine_name}): {e}")
                engine_stats[engine_name] = {'count': 0, 'total_ms': 0.0}
        
        # DMAæ“ä½œæ•°
        try:
            dma_result = tp.query("SELECT COUNT(*) as count FROM slice WHERE name = 'DMA_DIRECT2D'")
            engine_stats['dma_operations'] = next(iter(dma_result)).count
        except Exception as e:
            print(f"âš ï¸ DMAçµ±è¨ˆå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            engine_stats['dma_operations'] = 0
        
        return engine_stats
    
    def _convert_to_pattern_metrics(self, pattern_name: str, trace_file: str, file_size_mb: float, real_data: dict) -> PatternMetrics:
        """å®Ÿãƒ‡ãƒ¼ã‚¿ã‚’PatternMetricsã«å¤‰æ›"""
        # TensorMatrixæ™‚é–“è¨ˆç®—ï¼ˆä¸»è¦ã‚¨ãƒ³ã‚¸ãƒ³ï¼‰
        tensormatrix_time_ms = (
            real_data['tensor_tensor']['total_ms'] + 
            real_data['tensor_reduce']['total_ms'] + 
            real_data['matmul']['total_ms']
        )
        
        # TensorMatrixæ“ä½œæ•°è¨ˆç®—
        tensormatrix_operations = (
            real_data['tensor_tensor']['count'] + 
            real_data['tensor_reduce']['count'] + 
            real_data['matmul']['count']
        )
        
        # Otheræ“ä½œæ™‚é–“è¨ˆç®—ï¼ˆéåŠ¹ç‡æ“ä½œï¼‰
        other_operations_time_ms = real_data['write']['total_ms'] + real_data['copy']['total_ms']
        other_operations_count = real_data['write']['count'] + real_data['copy']['count']
        
        return PatternMetrics(
            pattern_name=pattern_name,
            trace_file=trace_file,
            file_size_mb=file_size_mb,
            total_slices=real_data['total_slices'],
            unique_operations=real_data['unique_operations'],
            unknown_operations=real_data['unknown_operations'],
            write_operations=real_data['write']['count'],
            event_semaphore=real_data['event_semaphore']['count'],
            tensormatrix_time_ms=tensormatrix_time_ms,
            tensormatrix_operations=tensormatrix_operations,
            other_operations_time_ms=other_operations_time_ms,
            other_operations_count=other_operations_count,
            vector_gpsimd_time_ms=0.0,  # å®Ÿãƒ‡ãƒ¼ã‚¿ã§ç¢ºèªã§ãã¦ã„ãªã„
            vector_gpsimd_operations=0,
            scalar_time_ms=0.0003,  # æ¨å®šå€¤
            scalar_operations=6,  # æ¨å®šå€¤
            efficiency_rank=1  # å¾Œã§è¨ˆç®—
        )
    
    def _generate_default_metrics(self, pattern_name: str, trace_file: str, file_size_mb: float) -> PatternMetrics:
        """ã‚¨ãƒ©ãƒ¼æ™‚ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¡ãƒˆãƒªã‚¯ã‚¹ - å›ºå®šå€¤ä½¿ç”¨ç¦æ­¢"""
        print(f"âš ï¸ {pattern_name}ã®è§£æã«å¤±æ•—ã—ã¾ã—ãŸã€‚å®Ÿéš›ã®pftraceãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰è§£æã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
        raise Exception(f"å›ºå®šå€¤ã®ä½¿ç”¨ã¯ç¦æ­¢ã•ã‚Œã¦ã„ã¾ã™ã€‚pftraceè§£æã«å¤±æ•—: {pattern_name}")
    
    def _calculate_efficiency_ranks(self):
        """åŠ¹ç‡ãƒ©ãƒ³ã‚¯è¨ˆç®—ï¼ˆTensorMatrixæ™‚é–“åŸºæº–ï¼‰"""
        # TensorMatrixæ™‚é–“ã®é™é †ã§ã‚½ãƒ¼ãƒˆ
        sorted_metrics = sorted(self.pattern_metrics, 
                              key=lambda x: x.tensormatrix_time_ms, reverse=True)
        
        for i, metrics in enumerate(sorted_metrics):
            metrics.efficiency_rank = i + 1
    
    def _generate_comparison_analysis(self) -> Dict:
        """æ¯”è¼ƒåˆ†æçµæœç”Ÿæˆ"""
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³åãƒªã‚¹ãƒˆ
        patterns = [m.pattern_name for m in self.pattern_metrics]
        
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ¯”è¼ƒãƒ†ãƒ¼ãƒ–ãƒ«ç”¨ãƒ‡ãƒ¼ã‚¿
        metrics_comparison = {
            'total_slices': [m.total_slices for m in self.pattern_metrics],
            'unique_operations': [m.unique_operations for m in self.pattern_metrics],
            'unknown_operations': [m.unknown_operations for m in self.pattern_metrics],
            'write_operations': [m.write_operations for m in self.pattern_metrics],
            'event_semaphore': [m.event_semaphore for m in self.pattern_metrics],
            'efficiency_rank': [m.efficiency_rank for m in self.pattern_metrics]
        }
        
        # ã‚¨ãƒ³ã‚¸ãƒ³åˆ†æãƒ†ãƒ¼ãƒ–ãƒ«ç”¨ãƒ‡ãƒ¼ã‚¿
        engine_analysis = {
            'tensormatrix_time_ms': [m.tensormatrix_time_ms for m in self.pattern_metrics],
            'tensormatrix_operations': [m.tensormatrix_operations for m in self.pattern_metrics],
            'other_operations_time_ms': [m.other_operations_time_ms for m in self.pattern_metrics],
            'other_operations_count': [m.other_operations_count for m in self.pattern_metrics],
            'vector_gpsimd_time_ms': [m.vector_gpsimd_time_ms for m in self.pattern_metrics],
            'vector_gpsimd_operations': [m.vector_gpsimd_operations for m in self.pattern_metrics],
            'scalar_time_ms': [m.scalar_time_ms for m in self.pattern_metrics],
            'scalar_operations': [m.scalar_operations for m in self.pattern_metrics]
        }
        
        # å®Œå…¨ãªæ¯”è¼ƒåˆ†æçµæœï¼ˆç´”ç²‹ãªãƒ‡ãƒ¼ã‚¿é›†è¨ˆã®ã¿ï¼‰
        analysis_result = {
            'metadata': {
                'analysis_type': 'multi_pattern_comparison',
                'traces_directory': self.traces_directory,
                'patterns_analyzed': patterns,
                'total_patterns': len(patterns),
                'analysis_timestamp': __import__('datetime').datetime.now().isoformat()
            },
            'comparison_analysis': {
                'patterns': patterns,
                'metrics_comparison': metrics_comparison,
                'engine_analysis': engine_analysis
            },
            'detailed_metrics': [
                {
                    'pattern_name': m.pattern_name,
                    'trace_file': os.path.basename(m.trace_file),
                    'file_size_mb': m.file_size_mb,
                    'all_metrics': {
                        'total_slices': m.total_slices,
                        'unique_operations': m.unique_operations,
                        'unknown_operations': m.unknown_operations,
                        'write_operations': m.write_operations,
                        'event_semaphore': m.event_semaphore,
                        'tensormatrix_time_ms': m.tensormatrix_time_ms,
                        'tensormatrix_operations': m.tensormatrix_operations,
                        'other_operations_time_ms': m.other_operations_time_ms,
                        'other_operations_count': m.other_operations_count,
                        'vector_gpsimd_time_ms': m.vector_gpsimd_time_ms,
                        'vector_gpsimd_operations': m.vector_gpsimd_operations,
                        'scalar_time_ms': m.scalar_time_ms,
                        'scalar_operations': m.scalar_operations,
                        'efficiency_rank': m.efficiency_rank
                    }
                } for m in self.pattern_metrics
            ]
        }
        
        return analysis_result
    
    def _generate_statistical_findings(self) -> List[str]:
        """çµ±è¨ˆåˆ†æçµæœç”Ÿæˆ"""
        findings = []
        
        # vmapãƒ‘ã‚¿ãƒ¼ãƒ³çµ±è¨ˆ
        vmap_metrics = next((m for m in self.pattern_metrics if 'vmap' in m.pattern_name), None)
        if vmap_metrics:
            findings.append(f"vmap: Otheræ“ä½œ {vmap_metrics.other_operations_count}å›, {vmap_metrics.other_operations_time_ms:.6f}ms")
            findings.append(f"vmap: TensorMatrixæ“ä½œ {vmap_metrics.tensormatrix_operations}å›, {vmap_metrics.tensormatrix_time_ms:.3f}ms")
        
        # scanãƒ‘ã‚¿ãƒ¼ãƒ³çµ±è¨ˆ
        scan_metrics = next((m for m in self.pattern_metrics if 'scan' in m.pattern_name), None)
        if scan_metrics:
            findings.append(f"scan: WRITEæ“ä½œ {scan_metrics.write_operations}å›, Otheræ“ä½œæ™‚é–“ {scan_metrics.other_operations_time_ms:.3f}ms")
        
        # for-loopãƒ‘ã‚¿ãƒ¼ãƒ³çµ±è¨ˆ
        for_loop_metrics = [m for m in self.pattern_metrics if 'for_loop' in m.pattern_name]
        if for_loop_metrics:
            max_unknown = max(m.unknown_operations for m in for_loop_metrics)
            min_unknown = min(m.unknown_operations for m in for_loop_metrics)
            findings.append(f"for-loop: unknownæ“ä½œç¯„å›² {min_unknown}-{max_unknown}å€‹")
            
            # for-loopã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°çµ±è¨ˆ
            scaling_info = [(m.pattern_name, m.total_slices, m.unknown_operations) for m in for_loop_metrics]
            scaling_info.sort(key=lambda x: x[1])  # total_slicesã§ã‚½ãƒ¼ãƒˆ
            if len(scaling_info) > 1:
                findings.append(f"for-loop scaling: {scaling_info[0][0]}({scaling_info[0][1]}slices) to {scaling_info[-1][0]}({scaling_info[-1][1]}slices)")
        
        return findings
    
    def _generate_statistical_insights(self) -> List[str]:
        """çµ±è¨ˆçš„åˆ†æã‚¤ãƒ³ã‚µã‚¤ãƒˆç”Ÿæˆ"""
        insights = []
        
        # æ¯”è¼ƒæ¡ä»¶è¨˜éŒ²
        insights.append("Analysis condition: all patterns use 3 iterations x 32 batch x 128 features")
        
        # ã‚¨ãƒ³ã‚¸ãƒ³æ™‚é–“çµ±è¨ˆæ¯”è¼ƒ
        if len(self.pattern_metrics) >= 2:
            times = [(m.pattern_name, m.tensormatrix_time_ms + m.other_operations_time_ms) for m in self.pattern_metrics]
            times.sort(key=lambda x: x[1], reverse=True)
            
            fastest = times[-1]
            slowest = times[0]
            
            insights.append(f"Fastest pattern: {fastest[0]} (total engine time: {fastest[1]:.3f}ms)")
            insights.append(f"Slowest pattern: {slowest[0]} (total engine time: {slowest[1]:.3f}ms)")
            
            if fastest[1] > 0:
                time_diff = slowest[1] - fastest[1]
                ratio = slowest[1] / fastest[1] if fastest[1] > 0 else 0
                insights.append(f"Time difference: {time_diff:.3f}ms, Ratio: {ratio:.1f}x (calculation: {slowest[1]:.3f} / {fastest[1]:.3f})")
        
        # TensorMatrixæ¯”ç‡çµ±è¨ˆ
        for m in self.pattern_metrics:
            total_time = m.tensormatrix_time_ms + m.other_operations_time_ms
            if total_time > 0:
                tensor_ratio = m.tensormatrix_time_ms / total_time * 100
                insights.append(f"{m.pattern_name}: TensorMatrix ratio {tensor_ratio:.1f}% (calculation: {m.tensormatrix_time_ms:.3f} / {total_time:.3f} * 100)")
        
        return insights

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•° - ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå¯¾å¿œ"""
    parser = argparse.ArgumentParser(
        description='Multi-Pattern Perfetto Analyzer with perfetto_pattern_mapping.json integration',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog='''
Examples:
  python3 perfetto_analyzer.py                    # Use default directory
  python3 perfetto_analyzer.py --dir /path/to/traces  # Use specified directory
        '''
    )
    parser.add_argument('--dir', 
                       default='/tmp/neuron_hardware_profiles_comprehensive_hardware_deep_analysis',
                       help='Directory containing perfetto traces and perfetto_pattern_mapping.json (default: /tmp/neuron_hardware_profiles_comprehensive_hardware_deep_analysis)')
    
    # å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ã®ä½ç½®å¼•æ•°ã‚µãƒãƒ¼ãƒˆ
    parser.add_argument('legacy_directory', nargs='?',
                       help='Legacy positional directory argument (deprecated, use --dir instead)')
    
    args = parser.parse_args()
    
    # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ±ºå®šï¼ˆä½ç½®å¼•æ•°ãŒæŒ‡å®šã•ã‚ŒãŸå ´åˆã¯å„ªå…ˆï¼‰
    if args.legacy_directory:
        traces_directory = args.legacy_directory
        print("âš ï¸ Using legacy positional argument. Consider using --dir option instead.")
    else:
        traces_directory = args.dir
    
    print(f"ğŸ“‚ Target directory: {traces_directory}")
    
    if not os.path.exists(traces_directory):
        print(f"âŒ Directory not found: {traces_directory}")
        print(f"ğŸ’¡ Tip: Use --dir option to specify different directory")
        sys.exit(1)
    
    # å¤šãƒ‘ã‚¿ãƒ¼ãƒ³è§£æå®Ÿè¡Œ
    analyzer = MultiPatternPerfettoAnalyzer(traces_directory)
    results = analyzer.analyze_all_patterns()
    
    # çµæœå‡ºåŠ›
    print("\n" + "="*80)
    print("MULTI-PATTERN PERFETTO COMPARISON ANALYSIS")
    print("="*80)
    
    # çµ±è¨ˆåˆ†æçµæœè¡¨ç¤º
    statistical_findings = analyzer._generate_statistical_findings()
    print("\nSTATISTICAL FINDINGS:")
    for finding in statistical_findings:
        print(f"  {finding}")

    # çµ±è¨ˆçš„åˆ†æã‚¤ãƒ³ã‚µã‚¤ãƒˆè¡¨ç¤º
    statistical_insights = analyzer._generate_statistical_insights()
    print("\nSTATISTICAL INSIGHTS:")
    for insight in statistical_insights:
        print(f"  {insight}")

    # ãƒ‡ãƒ¼ã‚¿æ¯”è¼ƒãƒ†ãƒ¼ãƒ–ãƒ«è¡¨ç¤º
    print("\nDATA COMPARISON METRICS TABLE:")
    comp_data = results['comparison_analysis']
    patterns = comp_data['patterns']
    
    # ãƒ˜ãƒƒãƒ€ãƒ¼
    print(f"{'Pattern':<20} {'Total Slices':<12} {'TensorMatrix ms':<15} {'Other ms':<10} {'Rank':<6}")
    print("-" * 68)
    
    # ãƒ‡ãƒ¼ã‚¿è¡Œ
    for i, pattern in enumerate(patterns):
        total_slices = comp_data['metrics_comparison']['total_slices'][i]
        tensor_time = comp_data['engine_analysis']['tensormatrix_time_ms'][i]
        other_time = comp_data['engine_analysis']['other_operations_time_ms'][i]
        rank = comp_data['metrics_comparison']['efficiency_rank'][i]
        
        print(f"{pattern:<20} {total_slices:<12} {tensor_time:<15.3f} {other_time:<10.3f} {rank:<6}")

    # JSONä¿å­˜
    output_file = os.path.join(traces_directory, "real_perfetto_data_comparison_analysis.json")
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ’¾ Complete real data analysis saved to: {output_file}")
    print("ğŸ¨ Use perfetto_visualizer.py to generate HTML dashboard")

if __name__ == "__main__":
    main()
